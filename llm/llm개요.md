# LLM 개요 

## LLM 이란
- large language model 의 약자로, 대규모 데이터 세트에서 훈련된 인공지능 언어 모델을 의미 
- 자연어 처리 작업에서 널리 사용 
- 텍스트 생성, 분류, 번역, 질문 응답 등의 기능 수행 

LLM 을 만들기 위한 것
1. 대규모 텍스트 데이터 
2. 대규모 컴퓨팅 리소스 (gpu)


### 트랜스포머 모델
llm 에 사용되는 모델 

트랜스포머 모델은 2017년 구글이 발표한 딥러닝 모델로, 어텐션(Attention) 메커니즘, 특히 셀프 어텐션을 핵심으로 사용하여 문장 같은 순차 데이터를 처리하며, 단어들의 관계와 맥락을 효과적으로 파악해 언어 모델(ChatGPT 등)의 혁신을 이끌고 있는 AI 모델

특징 
- 셀프 어텐션(Self-Attention): 문장 내 모든 단어 쌍의 중요도를 계산해 문맥을 파악하며, RNN의 한계를 극복
- 병렬 처리: RNN과 달리 시퀀스를 순차적으로 처리하지 않고 한 번에 처리하여 학습 속도를 크게 향상

RNN  이란?
시계열(시간에 따라 달라지는), 순서 데이터를 학습시키기에 좋은 뉴럴 네트워크 구조 
순환 신경망(RNN)은 순차적 데이터 또는 시계열 데이터로 훈련된 심층 신경망으로, 
순차적 입력을 기반으로 순차적 예측 또는 결론을 내릴 수 있는 머신 러닝(ML) 모델을 만든다

### LLM 학습의 기본원리 
언어모델 (Language Model)은 다음에 올 단어가 무엇인지를 예측 


### 파운데이션 모델
- 범용성이 높고 다양한 응용 분야에서 사용될 수 있는 대규모 머신 러닝 
- 언어 모델들이 이에 해당 (gpt,claude  ..)



### fine tuning
- transfer learning == fine tuning
- 이미 학습된 neural networks 의 파라미터를 새로운 task에 맞게 미세 조정 하는 것을 의미 
- 목적에 맞게 파인 튜닝 


### 할루시네이션 / 편향(bias)

편향
- 데이터로부터의 편향이 생길 수 있다. 
- 사용자의 의견이나 선입견으로 편향이 생길 수 있다. 

할루시네이션
- 통계적 특성에 기반한 언어 모델이기 때문에 현실과 다른 정보를 생성할 수 있음


## 토크나이징

토큰
- 텍스트를 구성하는 개별 단위 
- "나는", "바보다" 

토큰화
- 주어진 텍스트를 개별 토큰들로 분리하는 과정
- 기준 : 공백, 구두점, 특수 문자, 문맥 .. 

토큰의 종류 
- 토큰화의 기준에 따라 여러 종류 토큰 생성 가능
- 단어, 문장, 문단, 형태소 토큰
    - 형태소 : 언어의 의미를 가진 가장 작은 단위 ("바", "보")

토큰화의 중요성 
- 대부분의 nlp 작업에서 텍스트를 토큰화해서 각 토큰을 기반으로 처리 
    - nlp : 자연어 처리 

- 문장이나 텍스트의 구조와 의미를 파악하는 데 도움을 준다.
- 토큰과 토큰화는 Nlp 전 전처리 단계


자주 사용하는 토큰화 
- 단어 단위 토큰화 
- 문자 단위 토큰화
- 단어,문자 사이 중간단위 토큰화 

## 인컨텍스트 러닝

사전 학습과 미세 조정을 결합하는 동시에 학습 프로세스 중에 작업별 지침이나 프롬프트를 통합하는 새로운 접근 방식

- 추가적인 파인튜닝 없이도 인컨텍스트 러닝으로만 원하는 결과를 얻는 것이 가능 
- llm 에게 효과적인 정보를 얻어내는 프롬프트 엔지니어 기법 연구
- 제로샷, 원샷, 퓨샷 .. 


## 창발 능력 (emergent abilities)

큰 llm 모델 학습 과정에서 "특정 임계치"를 넘으면 기존의 작은 llm 모델에서 발생하지 않았던 새로운 능력들이 발현된다는 주장


## temperature

- 다음 토큰이 샘플링 시에 뽑힐 확률을 뾰족하게 혹은 평평하게 만들어 주는 방법 
- 언어 모델에서 다음 토큰의 확률 예측하는 계산식에서 tempature를 조절하여 토큰의 확률값 조절 가능 

- 온도가 높을 경우 다양성 있는 텍스트가 생성될 확률 높아짐 
    - 20% 25% 23% 22% ..
- 온도가 낮을 수록 가장 높았던 확률을 가진 데이터가 뽑힐 확률 증가 
    - 3% 70% 5% 4% .. 